{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY')  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt(messages, model='gpt-4o-mini', response_format=\"text\", temperature=0.1):\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            response_format={'type' : response_format},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    if response_format == 'json_object':\n",
    "        return json.loads(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train.json', 'test': 'test.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"hf://datasets/TableQAKit/WTQ/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json(\"hf://datasets/TableQAKit/WTQ/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(input: str):\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=input,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_column(column_name, column_data):\n",
    "    column_string = column_name + ' ' + ' '.join(column_data.apply(str).values)\n",
    "    return get_embedding(column_string)\n",
    "\n",
    "def get_column_embeddings(df):\n",
    "    embeddings = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Map each column in parallel and collect results\n",
    "        results = executor.map(process_column, df.columns, [df[col] for col in df.columns])\n",
    "    \n",
    "    # Combine column names with their corresponding embeddings\n",
    "    embeddings = dict(zip(df.columns, results))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row_data):\n",
    "    row_string = ' '.join(row_data.apply(str).values)\n",
    "    return get_embedding(row_string)\n",
    "\n",
    "def get_row_embeddings(df):\n",
    "    embeddings = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(process_row, [df.iloc[i] for i in range(len(df))])\n",
    "    embeddings = dict(zip(range(len(df)), results))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    v1, v2 = np.array(v1), np.array(v2)\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    magnitude = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    return dot_product / magnitude if magnitude != 0 else 0\n",
    "\n",
    "def get_table_score(df, q):\n",
    "    column_embeddings = get_column_embeddings(df)  # 각 열(column)의 임베딩\n",
    "    row_embeddings = get_row_embeddings(df)        # 각 행(row)의 임베딩\n",
    "    q_embedding = get_embedding(q)                 # 질의(query) 임베딩\n",
    "    \n",
    "    scores = pd.DataFrame(0.0, index=range(len(df)), columns=df.columns, dtype=float)\n",
    "\n",
    "    # 행(row) 점수 계산\n",
    "    for i in range(len(df)):\n",
    "        row_score = cosine_similarity(row_embeddings[i], q_embedding)\n",
    "        scores.iloc[i, :] += row_score  # 모든 열에 동일한 row score 추가\n",
    "\n",
    "    # 열(column) 점수 계산\n",
    "    for col in df.columns:\n",
    "        col_score = cosine_similarity(column_embeddings[col], q_embedding)\n",
    "        scores[col] += col_score  # 열마다 개별 column score 추가\n",
    "\n",
    "    return scores.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def generate_table_images(original_df, score_df, cmap='YlOrBr'):\n",
    "    def create_table_image(df, scores=None):\n",
    "        \"\"\"\n",
    "        df: DataFrame to render as table\n",
    "        scores: DataFrame of scores with the same shape as df\n",
    "        cmap: Colormap for score-based coloring\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Create table\n",
    "        table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "\n",
    "        # Style and color settings\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "\n",
    "        # Adjust column widths based on content\n",
    "        for col_idx in range(len(df.columns)):\n",
    "            table.auto_set_column_width([col_idx])  # Set width based on column content\n",
    "\n",
    "        if scores is not None:\n",
    "            norm = Normalize(vmin=scores.min().min(), vmax=scores.max().max())\n",
    "            if type(cmap) == str:\n",
    "                try:\n",
    "                    colormap = plt.colormaps[cmap]  # 최신 colormap 호출 방식\n",
    "                except:\n",
    "                    colormap = cmap\n",
    "            else:\n",
    "                colormap = cmap\n",
    "            for (i, j), cell in table.get_celld().items():\n",
    "                if i == 0:  # Header row\n",
    "                    cell.set_facecolor('#CCCCCC')\n",
    "                    cell.set_text_props(weight='bold')\n",
    "                else:\n",
    "                    color = colormap(norm(scores.iloc[i - 1, j]))\n",
    "                    cell.set_facecolor(color)\n",
    "                    cell.set_text_props(color='black')\n",
    "        else:\n",
    "            for (i, j), cell in table.get_celld().items():\n",
    "                if i == 0:\n",
    "                    cell.set_facecolor('#CCCCCC')\n",
    "                    cell.set_text_props(weight='bold')\n",
    "\n",
    "        # Save to buffer\n",
    "        img_buffer = io.BytesIO()\n",
    "        plt.savefig(img_buffer, format='jpeg', bbox_inches='tight', dpi=100)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Convert to Base64\n",
    "        img_buffer.seek(0)\n",
    "        img_base64 = base64.b64encode(img_buffer.read()).decode('utf-8')\n",
    "        img_buffer.close()\n",
    "        return img_base64\n",
    "\n",
    "    # Create images\n",
    "    highlighted_table_img = create_table_image(original_df, scores=score_df)\n",
    "    original_table_img = create_table_image(original_df)\n",
    "\n",
    "    return highlighted_table_img, original_table_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    index = 885 # 12\n",
    "\n",
    "    table1 = df_test.iloc[index].table\n",
    "    table1 = pd.DataFrame(table1['rows'], columns=table1['header'])\n",
    "\n",
    "    q1 = df_test.iloc[index].question\n",
    "    score = get_table_score(table1, q1)\n",
    "\n",
    "    print(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    'custom_white_yellow', ['#FFFFFF', '#FFFF00'], N=256\n",
    ")\n",
    "\n",
    "highlighted_table, original_table = generate_table_images(table1, score, cmap=custom_cmap)\n",
    "\n",
    "# Base64 문자열을 디코딩\n",
    "image_data = base64.b64decode(highlighted_table)\n",
    "\n",
    "# 디코딩된 이미지를 BytesIO 객체로 변환\n",
    "image = io.BytesIO(image_data)\n",
    "\n",
    "# Jupyter Notebook에서 이미지 표시\n",
    "display(Image(data=image.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_text(data):\n",
    "    prompt = f\"\"\"\n",
    "Answer the following question. Just give the answer, not the process.\n",
    "\n",
    "Table: {data['table']}\n",
    "\n",
    "Question: {data['question']}\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "    answer = get_gpt(messages=[{'role':'user', 'content': prompt}], response_format = 'text')\n",
    "    answer = answer.lower()\n",
    "    return data['seq_out'] in answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_image(data):\n",
    "    table = pd.DataFrame(data['table']['rows'], columns=data['table']['header'])\n",
    "    score = get_table_score(table, data['question'])\n",
    "    highlighted_table, original_table = generate_table_images(table, score, cmap=custom_cmap)\n",
    "    \n",
    "    messages1 = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Answer the following question. Just give the answer, not the process. \\nTable: \"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\" : f\"data:image/jpeg;base64,{original_table}\",\n",
    "                    \"detail\" : 'auto'\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": f\"Question: {data['question']}\\nAnswer:\"}\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    messages2 = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Answer the following question. Just give the answer, not the process. \\nTable: \"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\" : f\"data:image/jpeg;base64,{highlighted_table}\",\n",
    "                    \"detail\" : 'auto'\n",
    "                }\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": f\"Question: {data['question']}\\nAnswer:\"}\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    # get ansnwer in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        answer1 = executor.submit(get_gpt, messages1, response_format = 'text')\n",
    "        answer2 = executor.submit(get_gpt, messages2, response_format = 'text')\n",
    "\n",
    "    answer1 = answer1.result().lower()\n",
    "    answer2 = answer2.result().lower()\n",
    "\n",
    "    return data['seq_out'] in answer1, data['seq_out'] in answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_text(data, output_file='text_results.txt'):\n",
    "    correct = 0\n",
    "    with open(output_file, 'w') as file:\n",
    "        for i in range(len(data)):\n",
    "            is_correct = answer_with_text(data.iloc[i])\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            file.write(f'Row {i}: {is_correct}\\n')\n",
    "        file.write(f'Correct: {correct} ({correct / len(data)})')\n",
    "    return correct / len(data)\n",
    "\n",
    "def evaluate_with_image(data, output_file='image_results.txt'):\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    TF = 0\n",
    "    FT = 0\n",
    "    with open(output_file, 'w') as file:\n",
    "        for i in range(len(data)):\n",
    "            result1, result2 = answer_with_image(data.iloc[i])\n",
    "            correct1 += result1\n",
    "            correct2 += result2\n",
    "            file.write(f'Row {i}: {result1}, {result2}\\n')\n",
    "            if result1 and not result2:\n",
    "                TF += 1\n",
    "            elif not result1 and result2:\n",
    "                FT += 1\n",
    "        file.write(f'Non-Highlight: {correct1} ({100*correct1 / len(data)}%) Highlight: {correct2} ({100*correct2 / len(data)}%)\\n')\n",
    "        file.write(f'TF: {TF}, FT: {FT}')\n",
    "    return correct1 / len(data), correct2 / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval with text\n",
    "print(evaluate_with_text(df_test[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval with image\n",
    "print(answer_with_image(df_test.iloc[885]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(400, 500, 100):\n",
    "    print(evaluate_with_image(df_test[i:i+100], output_file=f'results/image_results_{i}_{i+99}.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
